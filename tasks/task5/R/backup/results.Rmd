---
title: "MSE Results"
subtitle: "Analysis"
author: "L Kell"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
vignette: >
  %\VignetteIndexEntry{FLife}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
github_document:
    mathjax: TRUE
tags: FLife
license: Creative Commons Attribution-ShareAlike 4.0 International Public License
---
#Here is a draft Rmd document that reads in all the data poor simulations so far.

I have this DB from my simulations. Some are covariates that I was able to control, i.e. species (spp), maximum size (linf), then I had my control rule (mp) What I want to do is work out what causes the stock biomass (stock) and catches (catch) to vary, and how could you monitor the stock.

I.e. we want a big stock but you cant count the fish in the sea, all we can do is observe things like mean size. If we fish too hard then only small fish are left. So stop fishing until there are more big fish!

So I want to do

    see how potential indicators are correlated with system state
    see how well the different control work for different objectives
    optimise control rules
    
    
There are several factors in the OM to evaluate

    Species
    life history parameters, i.e. k, linf, l50,...

and the MP

    procedure, i.e. VPA, biomass dynamic and 2 emprirical rules

To compare across spp and life history parameters we can use an MP implemented as an ICES category 1 stock assessment with the ICES advice rule MP, i.e. this is ICES's ideal.

Then how does the performance of the MPs compare, across the multiple objectives. We can first use a multivariate regression to compare rules across the OMs. The next step will be to optimise rules for uncertainty given multiple conflicting objectives. But 1st we need to have an idea of what affects what.

We have a standard MSE DB, both for mydas and the tuna MSEs. This contains all the OM covariates (i.e. the different uncertainties considered), the candidate MPs, and the summary statistics.

It can be used to summarise the experimental design of the MSE, and also to analyses the experiment that was run.

I agree there is probably no single utility but we can use this basic structure to explore the consequences of different utilities. For example we can define different stakeholder groups and their associated utilities e.g.

    ICES Advice Rule, based on PA and limit reference points without economics
    ICES AR + Fisheries need to return a profit
    Marine Stewardship Council Certification, i.e. need to be highly certain/likely that stock > limit reference point and is above or fluctuating around MSY levels. If that is achieved then there is a price premium
    All of above but with a reduction in inter-annual variability

All of these utilities can be expressed using the standard DB. This allows us to ask how to achieve them, i.e.

    does the "best" MP depend upon utility? i.e. is there potential conflict between stakeholders
    how would reduction of uncertainty help, e.g. reduction in index CV or better information on life histories parameters
    Would  Insurance mechanisms help to mediate economic risks in marine fisheries (Mumford  et al)

Also defining a utility function allows use to use machine learning to develop control rules. 


What we need to do is identify for each of mpb, empd and empp which set of hyperparameters represents the "best" performing model compared to xsa, There are multiple trade-offs, however, between safety, status, yield and variabilty. Also the "best" hyperparameters may vary by stock, i.e. we cant simply say  for empd set k1=a, k2=b and gamma=c.

So the questions are how to

    find the best set of hyperarameters for a class of MP
    choose the best MP, i.e. mpb, empp or empd

To do this we need to explore a wider range of hyperparameters than currently run in the MSE simulations, and we need a rule or utility function for weighting the performance of the MPs.

Hence the acceptance plot http://rpubs.com/laurie/430868

We reject any MP which does not maintain the stock above the biomass limit with an acceptable probability (safety), then reject any MP that does not maintain or recover the stock to the green quadrant in an agreed period (recovery time). Only after this do we look at yield and variability in yield. Using this example the best rule would be mpsbt1.5 or mspsbt1.7

Based on this procedure we can now revisit the MPs by stock and choose a best MP. A problem is that we may not have evaluated a sufficiently wide range of hyperparameters to actually find the best parameters. How do we do that?

Also we need to choose weightings for the utility


```{r knitr_init, echo=FALSE, results="hide"}
library(knitr)
## Global options
opts_chunk$set(cache     =TRUE,
               echo      =FALSE,
               eval      =TRUE,
               prompt    =FALSE,
               comment   =NA,
               message   =FALSE,
               warning   =FALSE,
               tidy      =TRUE,
               fig.height=10,
               fig.width =8,
               fig.path  ="../tex/results/",
               cache.path="../cache/results/")

options(digits=3)

iFig=0
```
```{r pkgs}
library(ggplot2)
library(reshape)
library(plyr)
library(dplyr)
library(FLCore)
library(ggplotFL)

library(RPostgreSQL)
library(DBI)
```
```{r, dir}
dirMy=dirname(dirname(FLife:::getScriptPath()))
#dirMy ="/home/laurence/Desktop/sea++/mydas/tasks/task4"
dirDat=file.path(dirMy,"data")
```

```{r}
mseStart=c("brill"=54,"turbot"=54,"ray"=60,"pollack"=56,"sprat"=52,"razor"=54,"lobster"=57)
  
drv  =dbDriver("PostgreSQL")  
conLK=dbConnect(drv,   
               host    ='wklife.csrzweaa3tbm.eu-west-2.rds.amazonaws.com',
               dbname  ='wklife',
               port    =5432,
               user    ='mydas',
               password='Yes_Meski')
dbListTables(conLK)
```

```{r}
xsa =dbGetQuery(conLK,"SELECT * from mydas_xsa")
mpb =dbGetQuery(conLK,"SELECT * from mydas_mpb")
empd=dbGetQuery(conLK,"SELECT * from mydas_empd")
empp=dbGetQuery(conLK,"SELECT * from mydas_empp")
```

```{r}
mseStart=c("brill"=54,"turbot"=54,"ray"=60,"pollack"=56,"sprat"=52,"razor"=54,"lobster"=57)

names(xsa)[ 7]="catchJuv"
names(mpb)[10]="catchJuv"
```

```{r}
mpmpb =expand.grid(ftar=unique(mpb$ftar),btrig=unique(mpb$btrig))
mpempp=expand.grid(k1  =unique(empp$k1), k2   =unique(empp$k2))
mpempd=expand.grid(k1  =unique(empd$k1), k2   =unique(empd$k2),gamma=unique(empd$gamma))

mpmpb =cbind(mp=paste("mpb", seq(dim(mpmpb )[1]),sep=""),mpmpb)
mpempp=cbind(mp=paste("empp",seq(dim(mpempp)[1]),sep=""),mpempp)
mpempd=cbind(mp=paste("empd",seq(dim(mpempd)[1]),sep=""),mpempd)

mpb =merge(mpb, mpmpb)
empp=merge(empp,mpempp)
empd=merge(empd,mpempd)

dat=rbind(cbind(xsa,mp="xsa"),
          mpb[ ,c(names(xsa),"mp")],
          empp[,c(names(xsa),"mp")],
          empd[,c(names(xsa),"mp")])

dat$minYear=mseStart[dat$spp]
dat$maxYear=mseStart[dat$spp]+30
dat=subset(dat,year>=minYear&year<=maxYear)

with(dat,table(spp,year))

results=transform(dat,year=year-minYear)[,1:23]
```

```{r, eval=FALSE}
library(rdrop2)

## get and save token
#token<-drop_auth()
#saveRDS(token, "Dropbox/mydasOMs/token.RDS")

drop_download(path='mydasOMs/turbot.RData',overwrite=T)
load("turbot.RData")
par=cbind(spp="turbot",model.frame(prior))

drop_download(path='mydasOMs/brill.RData',overwrite=T)
load("brill.RData")
par=rbind(par,cbind(spp="brill",model.frame(prior)))

drop_download(path='mydasOMs/ray.RData',overwrite=T)
load("ray.RData")
par=rbind(par,cbind(spp="ray",model.frame(prior)))

drop_download(path='mydasOMs/pollack.RData',overwrite=T)
load("pollack.RData")
par=rbind(par,cbind(spp="pollack",model.frame(prior)))

drop_download(path='mydasOMs/sprat.RData',overwrite=T)
load("sprat.RData")
par=rbind(par,cbind(spp="sprat",model.frame(prior)))

drop_download(path='mydasOMs/lobster.RData',overwrite=T)
load("lobster.RData")
par=rbind(par,cbind(spp="lobster",model.frame(prior)))

drop_download(path='mydasOMs/razor.RData',overwrite=T)
load("razor.RData")
par=rbind.fill(par,cbind(spp="razor",model.frame(prior)))

par$spp =ac(par$spp)
par$iter=ac(par$iter)

save(par,file="/home/laurence/Desktop/par.RData")
```

```{r}
load("/home/laurence/Desktop/par.RData")

results=merge(results,par,by=c("spp","iter"))
results=results[do.call(order,results[,c("spp","mp","iter","year")]),]

save(results,mpmpb,mpempp,empd,
     file="/home/laurence/Desktop/Dropbox/mydasOMs/results.RData",compress="xz")
```

```{r}
xsa. =dbGetQuery(conLK,"SELECT * from mydas_xsa_pm")
mpb. =dbGetQuery(conLK,"SELECT * from mydas_mpb_pm")
empd.=dbGetQuery(conLK,"SELECT * from mydas_empd_pm")
empp.=dbGetQuery(conLK,"SELECT * from mydas_empp_pm")

pm=rbind(cbind(mp="xsa",                                        xsa.),
         cbind(mp= with(mpb.,paste( "m_",btrig,ftar, sep="")),   mpb.)[,c(1:3,6:10)],
         cbind(mp= with(empp.,paste("p_",k1,k2,      sep="")),  empp.)[,c(1:3,6:10)],
         cbind(mp= with(empd.,paste("d_",k1,k2,gamma,sep="")),  empd.)[,c(1:3,7:11)])
rm(xsa.,mpb.,empp.,empd.)
```

```{r}
dat=melt(pm,id=c("mp","spp","iter"))
dat=melt(ddply(dat,.(spp,variable), with, t(coefficients(lm(value~-1+mp)))))
names(dat)[3]="mp"
ggplot(dat)+geom_point(aes(mp,value))+
  facet_grid(variable~spp,scale="free")+
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
```{r}
dat=melt(pm,id=c("mp","spp","iter"))
dat=melt(ddply(dat,.(spp,variable), with, t(coefficients(lm(value~mp))))[,-3])
names(dat)[3]="mp"
ggplot(subset(dat,spp!="ray"))+
  geom_hline(aes(yintercept=0),col="red")+
  geom_point(aes(mp,value))+
  facet_grid(variable~spp,scale="free")+
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
```{r, fig.width=10}
dat=melt(pm,id=c("mp","spp","iter")) 
dat=subset(dat,substr(ac(dat$mp),1,1)=="d")
dat=melt(ddply(dat,.(spp,variable), with, t(coefficients(lm(value~mp))))[,-3])
names(dat)[3]="mp"
ggplot(subset(dat,spp!="ray"))+
  geom_hline(aes(yintercept=0),col="red")+
  geom_point(aes(mp,value))+
  facet_grid(variable~spp,scale="free")+
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

R/virgin 
How many years until in green quadrant
Probability of staying green quadrant
P(runs)
Yield
Yield AAV



What is the most important variable

+ iter
+ spp
+ mp
+ k
+ linf
+ t0
+ l50

```{r}

```

\newpage
## Session Info

```{r}
sessionInfo()
```

## Software Versions

* `r version$version.string`
* FLCore: `r packageVersion('FLCore')`
* FLife:  `r packageVersion('FLife')`
* FLBRP:  `r packageVersion('FLBRP')`
* **Compiled**: `r date()`

## Author information

**Laurence Kell**. laurie@seaplusplus.es

## Acknowledgements

This vignette and many of the methods documented in it were developed under the MyDas project funded by the Irish exchequer and EMFF 2014-2020. The overall aim of MyDas is to develop and test a range of assessment models and methods to establish Maximum Sustainable Yield (MSY) reference points (or proxy MSY reference points) across the spectrum of data-limited stocks.

# References {#References}

Look at this example where utility=k1*k2 and k1 decreases utility and k2 increases it


```{r}
ggplot(melt(transform(data.frame(val=-10:10,
                                 k1 =2+(-10:10*-0.2),
                                 k2 =1+(-10:10* 0.5)),
                      utility=k1*k2),id=c("val")))+
geom_line(aes(val,value,col=variable))
```


```{r,fig.width=10}
ggplot(melt(transform(data.frame(val=-10:10,
                                 k1 =2+(-10:10*-0.5),
                                 k2 =1+(-10:10* 0.2)),
                      utility=k1*k2),id=c("val")))+
geom_line(aes(val,value,col=variable))
```

```{r}
dat      =subset(results,substr(mp,1,4)=="empd")
mpempd   =expand.grid(k1  =unique(empd$k1), k2=unique(empd$k2),gamma=unique(empd$gamma))
mpempd$mp=paste("empd",seq(1:18),sep="")
dat      =merge(dat,mpempd)

lm(rec/rec_hat~k1+gamma:spp,data=dat)
```

https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a